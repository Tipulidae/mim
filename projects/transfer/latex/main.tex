\documentclass[journal,twoside,web]{ieeecolor}
\usepackage{generic}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}
\usepackage{hyperref}
\hypersetup{hidelinks=true}
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\markboth{\hskip25pc IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS}
{Author \MakeLowercase{\textit{et al.}}: Title}
\begin{document}
\title{Transfer Learning for Predicting Acute Myocardial Infarction Using Electrocardiograms}
\author{Axel Nyström, Anders Björkelund, Mattias Ohlsson, Jonas Björk, Ulf Ekelund and Jakob Lundager Forberg
\thanks{Manuscript received ???. This work was part of the AIR Lund (Artificially Intelligent use of Registers at Lund University) research environment, and received funding from the Swedish Research Council (VR; grant no. 2019-00198). The study also received funding from the Swedish Heart-Lung Foundation (2018-0173) and Sweden's innovation agency (Vinnova; DNR 2018-0192). (\textit{Corresponding author: Axel Nyström}.)}
\thanks{Axel Nyström is with the Department of Laboratory Medicine, Lund University, Lund, Sweden (e-mail: axel.nystrom@med.lu.se).}
\thanks{Anders Björkelund is with the Center for Environmental and Climate Science, Lund University, Lund, Sweden (e-mail: anders.bjorkelund@cec.lu.se).}
\thanks{Mattias Ohlsson is with the Center for Environmental and Climate Science, Lund University, Lund, Sweden, and also with the Center for Applied Intelligent Systems Research (CAISR), Halmstad University, Halmstad, Sweden (e-mail: mattias.ohlsson@cec.lu.se).}
\thanks{Jonas Björk is with the Department of Laboratory Medicine, Lund University, Lund, Sweden, and also with Clinical Studies Sweden, Forum South, Skåne University Hospital, Lund, Sweden (e-mail: jonas.bjork@med.lu.se).}
\thanks{Ulf Ekelund is with the department of Internal and Emergency Medicine, Skåne University Hospital, Lund, Sweden, and also with the Department of Clinical Sciences, Lund University, Lund, Sweden.}
\thanks{Jakob Lundager Forberg is with the Department of Clinical Sciences, Lund University, Lund, Sweden, and also with the Department of Emergency Medicine, Helsingborg Hospital, Helsingborg, Sweden (e-mail: jakob.lundager-forberg@skane.se).}
}

\maketitle

\begin{abstract}
Should be 150 -- 250 words, doesn't have to be "structured". Should indicate the objective of the study, methods, major results, conclusions, and one sentence significance to biomedical research.
\end{abstract}

\begin{IEEEkeywords}
Machine learning, transfer learning, electrocardiography, cardiovascular diseases
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}
%\IEEEPARstart{W}{hen} it comes to machine learning, more data is almost always better. Large models are more capable of learning, but require more data to achieve good results than smaller models. In transfer learning, a model is first trained to solve a "source task" in a step called pretraining, and the pretrained model can then be finetuned to the "target" or downstream task. Transfer learning can be viewed as a way to increase the effective model size without having to obtain more data, or by exploiting other data sources that can't be otherwise used in the downstream learning task. The relationship between performance, model size, and data size depends on the details of the data and the task itself.

%In this work we explore this relationship and the interplay with supervised transfer learning, for the specific target task of using electrocardiograms (ECGs) to predict acute myocardial infarction (AMI) within 30 days of arriving at the emergency department (ED) with chest pain. 

%In this work we focus on the task of using electrocardiograms (ECGs) to predict acute myocardial infarction (AMI) within 30 days of arriving at the emergency department (ED) with chest pain, and we evaluate the performance in terms of AUC scores for three different ResNet 

%When it comes to machine learning, more data is almost always better. 

%[All models are better with more data. Larger models have greater potential, but require more data to unlock it. For any specific task there is thus a trade-off between model capacity and data availability. In practice it can be expensive to collect a large dataset that follows the required distribution, while datasets with similar but different distributions are more easily obtainable. Transfer learning is the methodology by which we seek to improve the target performance by use of such out-of-distribution "source" datasets.

More specifically, we train and evaluate four different convolutional neural networks (CNN): a simple baseline and three ResNet type models that achieved state of the art on various ECG related classification tasks from the recent literature. We compare the performance of each model on our downstream task (predicting AMI) both with and without pretraining. We consider different sizes of both the source and target datasets, as well as three different pretraining strategies.

Our target dataset contains 38631 ECGs (one ECG per ED chest pain episode) from 32892 consecutive ED chest pain patients, where the incidence of AMI was 5.8\%. The source dataset contains 877197 ECGs from 167458 consecutive non chest pain ED patients, from which we include a two year history of ECGs collected from any health-care visit in the region. The source dataset does not include any ECGs from any patients that have been included in the target dataset. All the ECGs are 12-lead 10s ECGs with a frequency of 500 Hz. 

Our results indicate a substantial improvement on the downstream task for all model types and pretraining strategies (a lift of 2-7 percentage points AUC), where the greatest improvements was seen on the three ResNet models. Although all ResNet models outperformed the baseline, only the xresnet1d-50, which is the smallest ResNet model in our study in terms of learnable parameters, was able to do so without the use of transfer learning.




Our results confirm that when labeled data is scarce and pretraining is not available, smaller models outperform bigger ones. Whereas all models are improved by pretraining, the larger models are improved more, and with enough source data to use for pretraining, they will overtake the smaller models. From the pretraining task under consideration, age regression works best, with the best model achieving a target AUC of 83\%, which is 8 percentage points better than the best model that is not using pretraining (baseline CNN, AUC 75\%).


% In this work we explore a simple, supervised pretraining scheme and the relationship between model size, source data size, target data size and classification performance for the specific task of using electrocardiograms (ECGs) to predict acute myocardial infarction (AMI) within 30 days of arriving at the emergency department (ED) with chest pain. 



% For a given dataset and task, one is faced with the challenge of selecting a model of appropriate size. When labeled data is scarce, transfer learning can sometimes be employed to increase the effective model size 

% one is faced with a balancing act between acquiring more data on the one hand, and
% A few things are well known to correlate with the performance of machine learning models in general. Increasing the amount of high-quality data is one of them. In supervised learning, we are primarily interested in labeled data, which tends to be more difficult and expensive to get, since it often requires manual work to annotate and verify the labels. In the absense of labeled data, one can instead make use of unlabeled data or data from a neighboring domain in what is known as transfer learning.

% Explanation of transfer learning.

% In this project, we explore the relationship between model complexity and data size when using ECGs to predict AMI within 30 days of arriving at the ED with chest pain. We train and evaluate three different ResNet models that achieved state of the art on various ECG related classification tasks and compare their performance on our downstream task both with and without pretraining. We consider different sizes of both the source (i.e. pretraining task) and target (downstream task) datasets, and compare different pretraining strategies. We also compare the ResNet models with a simple baseline CNN.

% The target dataset contains 38631 ECGs (one ECG per ED chest-pain episode) from 32892 consecutive ED patients, where the incidence of AMI was 5.8\%. The source dataset contains 877197k ECGs from 167458 consecutive non-chest pain ED patients. The source dataset does not include any ECGs from any patients that have been included in the target dataset. 

% Our results indicate that when labeled data is scarce and pretraining is not available, smaller models outperform bigger ones. Whereas all models are improved by pretraining, the larger models are improved more, and with enough source data to use for pretraining, they will overtake the smaller models. All the models were similarly improved by increasing the amount of target data. 

% As a final experiment, we 

% 

\section{Related work}
Strodthoff ResNet (xresnet50) \cite{mehari2022}

Ribeiro ResNet \cite{ribeiro2020}

Gustafsson ResNet \cite{gustafsson2022}

S4 models \cite{mehari2023}

PTB-XL \cite{wagner2020}

PTB-XL benchmarks and insights \cite{strodthoff2020}


% Strodthoff et al.
% Published the PTBXL dataset as well as a series of benchmarks
% Has investigated different self-supervised learning frameworks (CPC, BYOL and SimCLR) in \cite{mehari2022} as a means to alleviate the challenge of limited data availability. Compares the impact of self-supervised pretraining on finetuned ECG classifiers. Comprehensive assessment of self-supervised pretraining in the ECG domain carried out exclusively on publicly available datasets. 

% In \cite{mehari2022}, the authors indicate that self-supervised pretraining improves downstream performance, downstream data efficiency, and robustness of downstream classifiers (in particular, robustness against input perturbations). 
% The self-supervised strategies are orthogonal (I think) to the model architecture used. That is to say, one can pick a generic prediction model and plug it into any of the self-supervised strategies. In this work, the authors use an "xres-net architecture" and state that "xresnet1d50" is similar in performance to "xresnet1d101" which they used in a previous work (benchmarks and insights). As downstream task, they predict all 71 labels in PTBXL and evaluate the macro-averaged AUC. 

% In \cite{mehari2023} the authors introduce S4 (Structured State Space Sequence) models as a new type of model architecture, and use it in conjunction with self-supervised pretraining (in particular, CPC, which they showed in \cite{mehari2022} to perform best of a number of alternative self-supervised strategies). 
% Again, the authors use PTB-XL and focus on the comprehensive ECG classification tasks (71 PTBXL labels). They also show that introducing patient metadata further increases performance. Furthermore, the authors find that downsampling the ECG from 500Hz to 100Hz has no discernible effect on the results, although this result might not hold for individual labels. 


% What is linear evaluation performance??? It is the performance of freezing the model and using the learned representation directly, analogous I think to using the pre-trained model as a feature-extractor and feeding the features to a logistic regression model. 

% Idea: it might be interesting to use our models to also evaluate the performance on PTBXL. That would make things more easily comparable with the strodthoff papers. 
% Idea: Try to run some of our experiments on data downsampled to 100Hz (RN1 now uses 400Hz). Although this might require changing the model architecture to accomodate the change. 
% Idea: The Mehari/Strodthoff papers describe a finetuning strategy in which the model is divided into several parts (head, body, stem/encoder), and have different learning rate for each part. Like us, they begin by freezing everything except the classification head, but unlike us, they also unfreeze batch-normalization. Then after a set number of epochs, they unfreeze the rest, but with different learning rates. I can try this also maybe (looks like it is feasible with tensorflow_addons: tfa.optimizers.MultiOptimizer. 

\section{Methods}
\subsection{Data sources}
% Skåne Emergency Medicine (SEM) cohort
% 10s, 12-lead, 500Hz ECGs etc. Use only 8 leads. 

\subsubsection{Target data}
The target dataset contains the index ECG of patients arriving at the ED with chest-pain, and the label is the AMI diagnosis (ICD-10 code I21) within 30 days of arrival at the ED. 

The outcome label AMI within 30 days of arrival at the ED was defined as the I21 diagnosis being set according to either Statistics Sweden (Statistiska centralbyrån) or Swedeheart (a national quality register). 

Visits lacking ECGs of sufficient technical quality recorded within a (-2h, 3h) interval from ED arrival were excluded from the dataset (n=3431), together with patients not linked to Statistics Sweden (n=?). All patients in SEM were 18 years or older at the time of arrival at the ED.

After excluding patients without 

The target dataset D_t = X, y contains 10s 12-lead ECGs sampled at 500Hz as input-tensor X, and a final AMI diagnosis (ICD-10 code I21) within 30 days as the outcome label y. 
Each sample contains the index ECG of a patient arriving at the ED with chest pain, 

The target dataset is constructed from all the chest pain visits (37447 patients, 47087 visits) , 



The target dataset consists of ECGs from patients arriving at the ED with chest pain as primary complaint, and the label is the binary outcome of AMI within 30 days of arrival at the ED, defined as the I21 diagnosis being set according to either Statistics Sweden (Statistiska centralbyrån) or Swedeheart (a national quality register). We use a single ECG per patient visit, chosen as the first ECG within 3h of arrival, or if there is no such ECG, the last ECG within 2h prior to arrival. The final dataset contains 44370 visits from  37447 patients, where 3431 visits from 1947 patients were excluded due to missing ECGs of sufficient quality and 2717 visits from 1276 patients were excluded as a result of the temporal test split (see below). The target dataset is partitioned into training, validation and test sets, where the test set is further divided into a random and a temporal split, as follows. The temporal test split consists of the chronologically last visits from the dataset, such that 15\% (5618) of the patients in the whole target dataset are included. All visits from the patients in the temporal test split are then removed from the remainder of the target dataset, for a total of 2717 visits and 1276 patients excluded. The random test split as well as the training and validation splits are a random partition on the patient level, with 55\% of the patients in the training set, 15\% in the validation set and 15\% in the random test set. The splits and exclusion criteria are illustrated in figure \ref{}. 


% There are 47087 chest pain visits from 37447 patients. From these, we exclude 3431 visits (1947 patients) where there was no appropriate index ECG.
% The temporal test set contains visits from the 5618 (15%) final patients, chronologically sp
% The temporal test set is chosen as the (chronologically) last visits from the dataset, such that 15% of the patients are included. In this way, we obtain a temporal test set containing 6012 visits from 5618 patients. 1276 of those patients had a total of 2717 prior ED chest-pain episodes that were excluded from the dataset in order to keep a clean separation between our development and test sets. 
% The remaining dataset was further split into three parts using a random split at the patient level, so that all visits from each patient is entirely contained within one of the sets. Thus we obtained a training set containing 24916 visits from 20596 (55%) patients, a validation (development) set containing 6711 visits from 5616 (15%) patients, and a test-set divided into two parts, each containing 15% of the patients, for a total of 12743 visits from 11235 patients. 

\subsubsection{Source data}
The source dataset contains 836972 ECGs from 162903 patients in the SEM cohort. The dataset excludes all ECGs from all patients in the target dataset in order to minimize the interaction between the two datasets. The source dataset is split into a training and validation set using a random split on the patient level, with 5\% of the patients in the validation set and the rest in the training set. We did not hold out a test set for the source data since we only care about how the pretrained models perform on the downstream task, and not on estimating the generalizability of the pretraining task itself. For the supervised pretraining task, we use age and sex as outcomes. The splits and exclusion criteria are illustrated in figure \ref{}.  

\subsection{Models}
In this work we consider four different models: a simple CNN model from our prior research [ref] that serves as a baseline model, the xresnet-50 model used by \cite{strodthoff2020}

In this work we consider four different convolutional models applied to 10s, 12-lead ECGs described in the literature: a simple model from our own prior research [ref] that serves as a baseline model, together with three variations of residual neural networks (ResNets) of different size and complexity. The selected models (except our baseline model) achieve state-of-the-art performance on their respective tasks. We have not performed any parameter tuning for the models under consideration, with the exception of learning rate and number of training epochs. 

A brief overview of the models and their complexity in terms of number of parameters and convolutional layers is shown in table #. In the following sections we describe the origins and general structure of each model.

		  params	conv-layers	source
cnn-basic	   20479	2		Nyström et al.
xrn50		  892449	51		Strodthoff et al.
rn-ribeiro	 6784561	9		Ribeiro et al.
rn-gustafsson	33062569	25		Gustafsson et al.


\subsubsection{CNN baseline}
The best performing model from \cite{nystrom2024} using only the raw input ECG to predict MACE. Contains two convolutional layers followed by two fully connected layers. Although small in size compared to the other models, it should be noted that this model was tuned for a very similar task (predicting MACE within 30 days) on a very similar dataset (ESC-TROP, which is a subset of SEM).

\subsubsection{XRN}
The xresnet1d50 (XRN) model is an adaptation of the original resnet-50 model from \cite{he2016}, which also incorporates a number of tricks described by \cite{he2019}, adjusted by \cite{strodthoff2020} to work with ECG-data. The model consists of 51 convolutional layers arranged into four stages, where each stage further contains 3-6 residual block. The residual blocks each contain 3 convolutional layers and a shortcut connection. The xresnet1d50 model was found to outperform other resnet variations on the PTB-XL dataset for a variety of tasks. In particular, the model has been finetuned to perform well on the macro-AUC score of 71 different outcomes from PTB-XL, including AMI. 

The model also incorporates a type of data augmentation and ensembling. During training, the 10s input ECG is randomly cropped to a 2.5s long sequence (random sliding windows), while during validation and testing, the input ECG is sliced into 10 equidistant, overlapping 2.5s slices, and the predictions for all 10 slices are aggregated to a single prediction. We consider this augmentation/ensembling scheme as part of the model itself. The other models do not use augmentation or ensembling.

\subsubsection{RRN}
The Ribeiro ResNet (RRN) model builds on the architecture described by \cite{hannun2019} and was developed and tuned using over 2M ECGs to recognize six types of ECG abnormalities. The model consists of 9 convolutional layers arranged into 4 residual blocks. Compared to the XRN model, Ribeiro's version is "wide and shallow": each convolution uses more filters, has a larger kernel-size and downsamples the input signal less aggressively, but there are far fewer layers. 

The model downsamples the input ECG from 500Hz to 400Hz, and adds zero-padding to make each ECG 4096 samples along the time-axis. We adjusted it slightly to use only 8 leads instead of the full 12-lead input that was described in the article. 

\subsubsection{GRN}
The Gustafsson ResNet (GRN) model is an extension of the Ribeiro ResNet model, consisting of 25 convolutional layers arranged into 12 residual blocks. The main difference from the RRN model besides the number of layers is the addition of a Squeeze and Excite block within each residual block, which is intended to help weighting the channel-wise information \cite{hu2018}. Although still only using half the number of convolutions as the XRN model, the GRN model is by far the largest in terms of number of parameters at just over 33M, almost five times larger than RRN, 37 times larger than XRN, and 1600 times larger than the baseline CNN model.

Similarly to the RRN model, the input ECGs are downsampled to 400Hz and employs zero-padding to make the signal 4096 samples wide. 

\subsection{Transfer learning strategy}
Our goal is not to find the best possible transfer learning algorithm, but rather to quantify the benefit of transfer learning for some popular model architectures as they apply to our specific downstream task, and how that benefit depends on the number of data points (ECGs) in the pre-training and downstream tasks.

%to predicting AMI using ECGs from patients with chest pain, and to investigate the 

%To simplify the analysis, we use the same general transfer learning approach for each of the four models under consideration, and repeat the process of pre-training and finetuning on varying proportions of the available source and target data. 

To simplify the analysis, we use the same general transfer learning approach throughout the project, adjusting only the model architecture, pretraining target and amount of data in the pretraining and finetuning steps, as well as learning rate and number of epochs. 

The transfer learning strategy consists of three stages: In the first stage, a model is pretrained on the source dataset to predict age, sex or both. In the second stage, the final layer of the pretrained model is replaced by a small feed-forward neural network with randomized weights, and training proceeds on the target dataset to predict AMI. During the second stage, we only update the model weights of the last part of the model. In the third and last stage, finetuning continues and all model weights are allowed to be updated. 

For the first two stages we use a one-cycle learning rate schedule consisting of an initial linear warmup, followed by a cosine decay with a factor of 100, so the learning rate at the end is two orders of magnitude smaller than just after the warmup. The final stage uses a constant learning rate. 

For the target task we use binary crossentropy (BCE) as the loss function, and perform early stopping where the AUC on the validation set is maximized. For the source tasks we use BCE for sex prediction and mean absolute error (MAE) for age prediction. When predicting both age and sex simultaneously, we use a weighted sum of the MAE of age and BCE of sex as the loss, where the weight was chosen as 1 for BCE and 0.045 for AGE. The weights were chosen empirically from an early experiment such that the two target outcomes would be valued roughly the same. 




%Conceptually each model consists of three parts: a stem, body and head, as illustrated in image [ref]. Our transfer learning strategy is to first pretrain each model on one of the three source tasks, and then load the model weights from the epoch that had the best validation scores. We then replace the head with a small feed-forward neural network with randomized weights. We use the same architecture for the classification head for all models. The updated model is subsequently finetuned on the target task by initially only updating the weights in the head, and after a fixed number of epochs, . When training has converged, we "unfreeze" the remainder of the network and train for an additional 60-360 epochs. 

%We employ a one-cycle learning rate schedule for both pretraining and finetuning, which consists of an initial linear warmup followed by a cosine decay. During the finetuning, the learning-rate remains constant once the network is unfrozen. 

%All the models follow the described architecture and hyper-parameter settings as closely as possible, which means we have adapted (and refactored) code from the authors public code repositories where available. We have not further tuned any of the parameters for the models, with exception of batch-size and learning-rate, as well as the final classification head during finetuning of the pretrained models. 


\section{Results and Discussion}
% We achieve a mean absolute error (MAE) of 6.62 years for the age regression pretraining task, which is in line with previous studies (Lima \textit{et al.} achieves a MAE of 8.38 \cite{lima2021}. Strodthoff \textit{et al.} achieves a MAE of 6.86 years on PTB-XL for healthy subjects and 7.38 years for non-healthy subjects \cite{strodthoff2020}. Attia \textit{et al.} achieves a MAE of 6.9 years \cite{attia2019}.)

% The XRN model performs overall better than any of the other models, but we suspect that at least part of the success is owed to the augmentation/ensembling of the input. Indeed, if we adjust the model to not perform the random window slicing, the results are substantially worse (81 AUC with pretraining on age, ~75AUC without) and the training is much more unstable and prone to overfitting. More research would be required to map the effects of such augmentation schemes on Ribeiro-style ResNet models, and the effects of adjusting the size of Strodthoff-style models with respect to pretraining. 

% Main results (100% source data, 100% target data)
% Results on pretraining
% Results on finetuning
% Target data size
% Source data size (heatmap)

\subsection{Limitations}

\section{Summary and conclusions}
In this study we compared three different recently published state-of-the-art ResNet models and a simple baseline CNN model on the task of predicting AMI using ECGs. We explored the effects of a simple supervised transfer learning approach in which a separate collection of unrelated ECGs (lacking the AMI outcome label) were first pretrained to predict age and sex, and then finetuned to the target task of predicting AMI. This simple transfer learning scheme consistently improved our downstream predictions for all models, with the largest improvements for the largest models. 

The results of our study shows that transfer learning can lead to a performance increase of over 7 percentage points when classifying AMI using ECGs. We found that using age and sex as pretraining targets was a simple and effective strategy to make use of otherwise unlabelled ECGs, potentially obviating the need for more complicated unsupervised approaches. We discovered that the XRN architecture from Strodthoff et al outperformed the substantially larger ResNet models of Ribeiro et al and Gustafsson et al, both with and without transfer learning. Besides underlining the important lesson that larger models are not always better, (it also indicates that there are still plenty of opportunities for further improving the state-of-the-art in deep-learning based ECG classification. (needs more motivation probably)) 

We hope that our work can serve as an indication of how many ECGs are required for a given model type, or perhaps rather the appropriate model size for a given number of ECGs, and the relative lift one can expect from utilizing transfer learning in this context.

It is perhaps not surprising that more data means better predictions, but here we hope to at least indicate how much better a model will perform in the presence of increasing amounts of data, particularly when that additional data is in the form of more easily obtained "unlabelled" data from a different population. More sophisticated transfer learning schemes might perform better, but here at least we have set a lower bound. 






Large models do not necessarily perform well in the absense of large datasets, but when larger datasets from other domains are available, the effective model size can be increased with the help of transfer learning. But even with a source dataset of 800k unlabelled ECGs, there is still a limit to how large the model can become before it is outperformed by smaller architectures. 

Using age and sex as pretraining targets is a simple and effective strategy to make use of otherwise unlabelled ECGs, and should be feasible in most situations where ECGs are collected. 


\bibliographystyle{IEEEtran}
\bibliography{main}

\end{document}

